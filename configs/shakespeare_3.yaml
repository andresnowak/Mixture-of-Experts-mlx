# Training Configuration
training:
  learning_rate: 0.0005
  epochs: 1
  batch_size: 128
  sequence_length: 256
  expert_level_balance: 0.01

# Data Configuration
data:
  source_file: "dataset/shakespeare.txt"

# Model Configuration
model:
  type: "MoEDecoderTransformer"
  architecture:
    max_length: 256  # Same as sequence_length
    vocabulary_dimension: null  # Will be set from dataset.vocab_size
    embedding_dimension: 128
    attention_heads: 8
    num_layers: 4
    feedforward_dimension: 512  # 128 * 4
    shared_experts: 1
    routed_experts: 4
    top_k_routers: 2
