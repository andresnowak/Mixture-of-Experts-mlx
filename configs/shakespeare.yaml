# Training Configuration
training:
  learning_rate: 0.0005
  epochs: 1
  batch_size: 64
  sequence_length: 128

# Data Configuration
data:
  source_file: "dataset/shakespeare.txt"

# Model Configuration
model:
  type: "DecoderTransformer"
  architecture:
    max_length: 128  # Same as sequence_length
    vocabulary_dimension: null  # Will be set from dataset.vocab_size
    embedding_dimension: 128
    attention_heads: 8
    num_layers: 4
    feedforward_dimension: 512  # embedding_dimension * 4
